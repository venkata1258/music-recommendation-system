# -*- coding: utf-8 -*-
"""music recommendation system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TxinSuCB494uIGIBbENcNXbQb35eeCsj
"""

# Install necessary libraries
!pip install pandas numpy matplotlib seaborn scikit-learn surprise

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from surprise import Dataset, Reader, SVD
from surprise.model_selection import train_test_split, GridSearchCV
from surprise import accuracy

# Load the dataset
df_songs = pd.read_csv('/content/songdata.csv')
df_songs.head()

# Create a mock user-item interaction dataset
# For demonstration, we will assume 10 users and randomly assign listen counts to songs

num_users = 10
# Use df_songs instead of song
num_song = len(df_songs)

# Create a DataFrame to simulate user listening history
user_ids = [f'user_{i}' for i in range(1, num_users + 1)]
# Use df_songs instead of song
song_ids = df_songs['song'].tolist()

# Create a random listen count for each user-song pair
data = {
    'user_id': np.random.choice(user_ids, size=num_song * num_users),
    'song': np.tile(song_ids, num_users),
    'listen_count': np.random.randint(1, 10, size=num_song * num_users)  # Random listen counts between 1 and 10
}

df_user_song = pd.DataFrame(data)
print(df_user_song.head())  # Display the first few rows of the mock dataset

# Define a Reader object
reader = Reader(rating_scale=(1, 10))

# Load the dataset into Surprise
data = Dataset.load_from_df(df_user_song[['user_id', 'song', 'listen_count']], reader)

#Split the Data
from surprise.model_selection import train_test_split

# Split the dataset into training and testing sets (75% train, 25% test)
trainset, testset = train_test_split(data, test_size=0.25)

# Display the size of the training and testing sets
print(f"Number of training instances: {trainset.n_ratings}") # Use n_ratings attribute
print(f"Number of testing instances: {len(testset)}")

#Build the Recommendation Model
from surprise import SVD

# Create an SVD model
model = SVD()

# Train the model on the training set
model.fit(trainset)

#Evaluate the Model
from surprise import accuracy

# Make predictions on the test set
test_predictions = model.test(testset)

# Calculate and print RMSE
rmse = accuracy.rmse(test_predictions)
print(f"RMSE: {rmse}")

# Make Recommendations
def get_recommendations(user_id, n_recommendations=5):
    # Get all song ids
    all_song_ids = df_user_song['song'].unique()

    # Predict ratings for all songs for the user
    predictions = [model.predict(user_id, song) for song in all_song_ids]

    # Sort predictions by estimated rating
    predictions.sort(key=lambda x: x.est, reverse=True)

    # Get the top n recommendations
    top_n = predictions[:n_recommendations]

    # Return the recommended song titles and their estimated ratings
    recommended_songs = [(pred.iid, pred.est) for pred in top_n]
    return recommended_songs

# Example usage
user_id = 'user_1'  # Change this to any user_id you want to get recommendations for
recommendations = get_recommendations(user_id)
print(f"Recommendations for {user_id}:")
for song, rating in recommendations:
    print(f"Song: {song}, Estimated Rating: {rating:.2f}")